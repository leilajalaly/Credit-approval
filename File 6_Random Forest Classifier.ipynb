{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the dataset with its targete\n",
    "df_numeric = pd.read_csv('data_numeric_norm.csv')\n",
    "df_target=pd.read_csv('data_target.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SMOTE technique to balance dataset\n",
    "Y = df_target['TARGET'].astype('int')\n",
    "X_balance,Y_balance = SMOTE().fit_sample(df_numeric,Y)\n",
    "X_balance = pd.DataFrame(X_balance, columns = df_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 38)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_balance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes and class distributions for train/test data\n",
      "Shape train_data (600, 38)\n",
      "Shape test_data (1400, 38)\n",
      "Train data number of 0s 297 and 1s 303\n",
      "Test data number of 0s 703 and 1s 697\n"
     ]
    }
   ],
   "source": [
    "#split the data to train and test\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "    X_balance, Y_balance, test_size=0.7, random_state=112\n",
    ")\n",
    "train_targets = train_targets.values\n",
    "test_targets = test_targets.values\n",
    "print(\"Sizes and class distributions for train/test data\")\n",
    "print(\"Shape train_data {}\".format(train_data.shape))\n",
    "print(\"Shape test_data {}\".format(test_data.shape))\n",
    "print(\n",
    "    \"Train data number of 0s {} and 1s {}\".format(\n",
    "        np.sum(train_targets == 0), np.sum(train_targets == 1)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Test data number of 0s {} and 1s {}\".format(\n",
    "        np.sum(test_targets == 0), np.sum(test_targets == 1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the basic Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is 0.5828571428571429\n",
      "     0    1\n",
      "0  421  282\n",
      "1  302  395\n"
     ]
    }
   ],
   "source": [
    "#Create the random forest and evaluate classifier on our dataset(sample); Find accuracy and confusion matrix\n",
    "rf = RandomForestClassifier()\n",
    "rffit = rf.fit(train_data, train_targets)\n",
    "y_predict = rffit.predict(test_data)\n",
    "\n",
    "print('Accuracy Score is',accuracy_score(test_targets, y_predict))\n",
    "print(pd.DataFrame(confusion_matrix(test_targets,y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 10 fold cross-validation \n",
    "rf_cv_score = cross_val_score(rf, X_balance, Y_balance, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[421 282]\n",
      " [302 395]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59       703\n",
      "           1       0.58      0.57      0.57       697\n",
      "\n",
      "    accuracy                           0.58      1400\n",
      "   macro avg       0.58      0.58      0.58      1400\n",
      "weighted avg       0.58      0.58      0.58      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print confusion matrix and classification report\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_targets, y_predict))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_targets, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunning hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\leila-gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:282: UserWarning: The total space of parameters 90 is smaller than n_iter=100. Running 90 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_features': 'log2', 'max_depth': 500}\n"
     ]
    }
   ],
   "source": [
    "#Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 1000, num = 5)]\n",
    "\n",
    "#Number of features at every split\n",
    "max_features = ['auto', 'sqrt','log2','int','float','None']\n",
    "\n",
    "#Maximum depth\n",
    "max_depth = [int(x) for x in np.linspace(10, 500, num =2)]\n",
    "max_depth.append(None)\n",
    "\n",
    "#Create random grid\n",
    "random_grid = {'n_estimators': n_estimators,'max_features': max_features,'max_depth': max_depth }\n",
    "\n",
    "#Random search of parameters\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=0, random_state=2, n_jobs = -1)\n",
    "\n",
    "#Fit the model\n",
    "rf_random.fit(train_data, train_targets)\n",
    "\n",
    "#Print the results\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[422 281]\n",
      " [293 404]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.60      0.60       703\n",
      "           1       0.59      0.58      0.58       697\n",
      "\n",
      "    accuracy                           0.59      1400\n",
      "   macro avg       0.59      0.59      0.59      1400\n",
      "weighted avg       0.59      0.59      0.59      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Running Random Forest with the best parameters and find confusion matrix and classification report\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=500, max_features='log2')\n",
    "rf.fit(train_data,train_targets)\n",
    "y_predict = rf.predict(test_data)\n",
    "rf_cv_score = cross_val_score(rf, X_balance, Y_balance, cv=10)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_targets, y_predict))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_targets, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899322132842368"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find f1 score for above prediction\n",
    "f1_score(test_targets, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC on test set:\n",
      "\n",
      "0.5899557338808263\n"
     ]
    }
   ],
   "source": [
    "test_predictions = y_predict\n",
    "test_predictions_class = ((test_predictions > 0.5) * 1).flatten()\n",
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Predicted_score\": test_predictions.flatten(),\n",
    "        \"Predicted_class\": test_predictions_class,\n",
    "        \"True\": test_targets,\n",
    "    }\n",
    ")\n",
    "roc_auc = metrics.roc_auc_score(test_targets, y_predict)\n",
    "print(\"The AUC on test set:\\n\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899557338808263"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(test_targets, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899954060538002"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(test_targets, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "leila-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
