{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling all files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the four csv files \n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "installments_payments = pd.read_csv('installments_payments.csv')\n",
    "credit_card_balance = pd.read_csv('credit_card_balance.csv')\n",
    "previous_application = pd.read_csv('previous_application.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data and Merging them together based on IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove instances with at least one empty column \n",
    "data_train = data_train.dropna(axis=0)\n",
    "installments_payments = installments_payments.dropna(axis=0)\n",
    "credit_card_balance = credit_card_balance.dropna(axis=0)\n",
    "previous_application = previous_application.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data_train, credit_card_balance\n",
    "Data_train_credit=pd.merge(data_train,credit_card_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Data_train_credit with previous_application\n",
    "data_train_app=pd.merge(Data_train_credit,previous_application, on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge data_train_app with installments_payments to create final data\n",
    "data=pd.merge(data_train_app,installments_payments, on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove previous ID\n",
    "del data['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep one information per applicant\n",
    "data = data.drop_duplicates(subset='SK_ID_CURR', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a glamps of our data\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the index of dataset and make it start from 0 to the end\n",
    "data.index = range(9390)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negetive number doesnt have meaning in our dataset(like days of birth and employment) so make all negetive to positive\n",
    "data = data.apply(lambda x: x.abs() if np.issubdtype(x.dtype, np.number) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of TARGET columns\n",
    "data['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw the ratio of each class to whole data\n",
    "fig, axs = plt.subplots(1,2,figsize=(8,4))\n",
    "sns.countplot(x='TARGET',data=data,ax=axs[0])\n",
    "axs[0].set_title(\"Frequency of each Loan Status\")\n",
    "data.TARGET.value_counts().plot(x=None,y=None, kind='pie', ax=axs[1],autopct='%1.2f%%')\n",
    "axs[1].set_title(\"Percentage of each Loan status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine classes of all columns\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique classes in each object columns\n",
    "data.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding of categorical variables(make all object columns to numeric)\n",
    "data = pd.get_dummies(data)\n",
    "print('Training Features shape: ', data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine dtypes of data\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliear in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SK_ID_CURR'] = data['SK_ID_CURR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eximine the destribution of features\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns that need to be chase for outliears\n",
    "Outliear_detection=data[[\n",
    "    'AMT_INCOME_TOTAL','AMT_CREDIT_x','AMT_ANNUITY_x','AMT_GOODS_PRICE_x','DAYS_EMPLOYED','DAYS_BIRTH'\n",
    "                        , 'AMT_BALANCE','AMT_CREDIT_LIMIT_ACTUAL', 'AMT_TOTAL_RECEIVABLE','AMT_PAYMENT_TOTAL_CURRENT'\n",
    "                        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using zscore to deal with outliears\n",
    "from scipy import stats\n",
    "data=data[(np.abs(stats.zscore(Outliear_detection)) <= 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many rows exist after deleting outliears\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making target value as a seperate dataframe\n",
    "data_target=data[['TARGET']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of approved(0) and rejected(1) applicants\n",
    "data_target['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the whole dataset called data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply MinMaxScaler to have a uniform dataset\n",
    "\n",
    "data1_float=data1.values.astype(float)\n",
    "#Create a min_max processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "#Create an object to transform the data to fit min_max processor\n",
    "data1_float_transformed = min_max_scaler.fit_transform(data1_float)\n",
    "\n",
    "#Save the transformed data as a DataFrame\n",
    "data1= pd.DataFrame(data1_float_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data1 and data_target for the advance use\n",
    "data1.to_csv (r'C:\\Users\\User\\Desktop\\A_Thesis\\Dataset\\Dataset_CSV\\data1.csv', index = False, header=True)\n",
    "data_target.to_csv (r'C:\\Users\\User\\Desktop\\A_Thesis\\Dataset\\Dataset_CSV\\data_target1.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "leila-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
